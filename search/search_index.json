{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to HttpRunner's documentation!\n\n\n\u70b9\u51fb\u6b64\u5904\u67e5\u770b\n\u4e2d\u6587\u4f7f\u7528\u8bf4\u660e\u6587\u6863\n\u3002",
            "title": "Home"
        },
        {
            "location": "/Introduction/",
            "text": "Introduction\n\n\nDesign Philosophy\n\n\nTake full reuse of Python's existing powerful libraries: \nRequests\n, \nunittest\n and \nLocust\n. And achieve the goal of API automation test, production environment monitoring, and API performance test, with a concise and elegant manner.\n\n\nKey Features\n\n\n\n\nInherit all powerful features of \nRequests\n, just have fun to handle HTTP in human way.\n\n\nDefine testcases in YAML or JSON format in concise and elegant manner.\n\n\nSupports \nfunction\n/\nvariable\n/\nextract\n/\nvalidate\n mechanisms to create full test scenarios.\n\n\nWith \ndebugtalk.py\n plugin, module functions can be auto-discovered in recursive upward directories.\n\n\nTestcases can be run in diverse ways, with single testset, multiple testsets, or entire project folder.\n\n\nTest report is concise and clear, with detailed log records. See \nPyUnitReport\n.\n\n\nWith reuse of \nLocust\n, you can run performance test without extra work.\n\n\nCLI command supported, perfect combination with \nJenkins\n.\n\n\n\n\nLearn more\n\n\nYou can read this \nblog\n to learn more about the background and initial thoughts of \nHttpRunner\n.",
            "title": "Introduction"
        },
        {
            "location": "/Introduction/#introduction",
            "text": "",
            "title": "Introduction"
        },
        {
            "location": "/Introduction/#design-philosophy",
            "text": "Take full reuse of Python's existing powerful libraries:  Requests ,  unittest  and  Locust . And achieve the goal of API automation test, production environment monitoring, and API performance test, with a concise and elegant manner.",
            "title": "Design Philosophy"
        },
        {
            "location": "/Introduction/#key-features",
            "text": "Inherit all powerful features of  Requests , just have fun to handle HTTP in human way.  Define testcases in YAML or JSON format in concise and elegant manner.  Supports  function / variable / extract / validate  mechanisms to create full test scenarios.  With  debugtalk.py  plugin, module functions can be auto-discovered in recursive upward directories.  Testcases can be run in diverse ways, with single testset, multiple testsets, or entire project folder.  Test report is concise and clear, with detailed log records. See  PyUnitReport .  With reuse of  Locust , you can run performance test without extra work.  CLI command supported, perfect combination with  Jenkins .",
            "title": "Key Features"
        },
        {
            "location": "/Introduction/#learn-more",
            "text": "You can read this  blog  to learn more about the background and initial thoughts of  HttpRunner .",
            "title": "Learn more"
        },
        {
            "location": "/Installation/",
            "text": "Installation\n\n\nHttpRunner\n is available on \nPyPI\n and can be installed through pip or easy_install.\n\n\n$ pip install HttpRunner\n\n\n\n\nor\n\n\n$ easy_install HttpRunner\n\n\n\n\nIf you want to keep up with the latest version, you can install with github repository url.\n\n\n$ pip install git+https://github.com/HttpRunner/HttpRunner.git#egg=HttpRunner\n\n\n\n\nUpgrade\n\n\nIf\b you have installed \nHttpRunner\n before and want to upgrade to the latest version, you can use the \n-U\n option.\n\n\nThis option works on each installation method described above.\n\n\n$ pip install -U HttpRunner\n$ easy_install -U HttpRunner\n$ pip install -U git+https://github.com/HttpRunner/HttpRunner.git#egg=HttpRunner\n\n\n\n\nCheck Installation\n\n\nWhen HttpRunner is installed, a \nhttprunner\n (\nhrun\n for short) command should be available in your shell (if you're not using\nvirtualenv\u2014which you should\u2014make sure your python script directory is on your path).\n\n\nTo see \nHttpRunner\n version:\n\n\n$ httprunner -V     # same as: hrun -V\nHttpRunner version: 0.8.1b\nPyUnitReport version: 0.1.3b\n\n\n\n\nTo see available options, run:\n\n\n$ httprunner -h     # same as: hrun -h\nusage: httprunner [-h] [-V] [--log-level LOG_LEVEL] [--report-name REPORT_NAME]\n        [--failfast] [--startproject STARTPROJECT]\n        [testset_paths [testset_paths ...]]\n\nHttpRunner.\n\npositional arguments:\ntestset_paths         testset file path\n\noptional arguments:\n-h, --help            show this help message and exit\n-V, --version         show version\n--log-level LOG_LEVEL\n                        Specify logging level, default is INFO.\n--report-name REPORT_NAME\n                        Specify report name, default is generated time.\n--failfast            Stop the test run on the first error or failure.\n--startproject STARTPROJECT\n                        Specify new project name.\n\n\n\n\nSupported Python Versions\n\n\nHttpRunner supports Python 2.7, 3.4, 3.5, and 3.6. And we strongly recommend you to use \nPython 3.6\n.\n\n\nHttpRunner\n has been tested on \nmacOS\n, \nLinux\n and \nWindows\n platforms.",
            "title": "Installation"
        },
        {
            "location": "/Installation/#installation",
            "text": "HttpRunner  is available on  PyPI  and can be installed through pip or easy_install.  $ pip install HttpRunner  or  $ easy_install HttpRunner  If you want to keep up with the latest version, you can install with github repository url.  $ pip install git+https://github.com/HttpRunner/HttpRunner.git#egg=HttpRunner",
            "title": "Installation"
        },
        {
            "location": "/Installation/#upgrade",
            "text": "If\b you have installed  HttpRunner  before and want to upgrade to the latest version, you can use the  -U  option.  This option works on each installation method described above.  $ pip install -U HttpRunner\n$ easy_install -U HttpRunner\n$ pip install -U git+https://github.com/HttpRunner/HttpRunner.git#egg=HttpRunner",
            "title": "Upgrade"
        },
        {
            "location": "/Installation/#check-installation",
            "text": "When HttpRunner is installed, a  httprunner  ( hrun  for short) command should be available in your shell (if you're not using\nvirtualenv\u2014which you should\u2014make sure your python script directory is on your path).  To see  HttpRunner  version:  $ httprunner -V     # same as: hrun -V\nHttpRunner version: 0.8.1b\nPyUnitReport version: 0.1.3b  To see available options, run:  $ httprunner -h     # same as: hrun -h\nusage: httprunner [-h] [-V] [--log-level LOG_LEVEL] [--report-name REPORT_NAME]\n        [--failfast] [--startproject STARTPROJECT]\n        [testset_paths [testset_paths ...]]\n\nHttpRunner.\n\npositional arguments:\ntestset_paths         testset file path\n\noptional arguments:\n-h, --help            show this help message and exit\n-V, --version         show version\n--log-level LOG_LEVEL\n                        Specify logging level, default is INFO.\n--report-name REPORT_NAME\n                        Specify report name, default is generated time.\n--failfast            Stop the test run on the first error or failure.\n--startproject STARTPROJECT\n                        Specify new project name.",
            "title": "Check Installation"
        },
        {
            "location": "/Installation/#supported-python-versions",
            "text": "HttpRunner supports Python 2.7, 3.4, 3.5, and 3.6. And we strongly recommend you to use  Python 3.6 .  HttpRunner  has been tested on  macOS ,  Linux  and  Windows  platforms.",
            "title": "Supported Python Versions"
        },
        {
            "location": "/quickstart/",
            "text": "QuickStart\n\n\nIntroduction to Sample Interface Service\n\n\nAlong with this project, I devised a sample interface service, and you can use it to familiarize how to play with \nHttpRunner\n.\n\n\nThis sample service mainly has two parts:\n\n\n\n\nAuthorization, each request of other APIs should sign with some header fields and get token first.\n\n\nRESTful APIs for user management, you can do CRUD manipulation on users.\n\n\n\n\nAs you see, it is very similar to the mainstream production systems. Therefore once you are familiar with handling this demo service, you can master most test scenarios in your project.\n\n\nLaunch Sample Interface Service\n\n\nThe demo service is a flask server, we can launch it in this way.\n\n\n$ export FLASK_APP=tests/api_server.py\n$ flask run\n * Serving Flask app \"tests.api_server\"\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n\n\n\n\nNow the sample interface service is running, and we can move on to the next step.\n\n\nCapture HTTP request and response\n\n\nBefore we write testcases, we should know the details of the API. It is a good choice to use a web debugging proxy tool like \nCharles Proxy\n to capture the HTTP traffic.\n\n\nFor example, the image below illustrates getting token from the sample service first, and then creating one user successfully.\n\n\n\n\n\n\nAfter thorough understanding of the APIs, we can now begin to write testcases.\n\n\nWrite the first test case\n\n\nOpen your favorite text editor and you can write test cases like this.\n\n\n- test:\n    name: get token\n    request:\n        url: http://127.0.0.1:5000/api/get-token\n        method: POST\n        headers:\n            user_agent: iOS/10.3\n            device_sn: 9TN6O2Bn1vzfybF\n            os_platform: ios\n            app_version: 2.8.6\n        json:\n            sign: 19067cf712265eb5426db8d3664026c1ccea02b9\n\n- test:\n    name: create user which does not exist\n    request:\n        url: http://127.0.0.1:5000/api/users/1000\n        method: POST\n        headers:\n            device_sn: 9TN6O2Bn1vzfybF\n            token: F8prvGryC5beBr4g\n        json:\n            name: \"user1\"\n            password: \"123456\"\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 201}\n        - {\"check\": \"content.success\", \"comparator\": \"eq\", \"expect\": true}\n\n\n\n\nAs you see, each API request is described in a \ntest\n block. And in the \nrequest\n field, it describes the detail of HTTP request, includes url, method, headers and data, which are in line with the captured traffic.\n\n\nYou may wonder why we use the \njson\n field other than \ndata\n. That's because the post data is in \nJSON\n format, when we use \njson\n to indicate the post data, we do not have to specify \nContent-Type\n to be \napplication/json\n in request headers or dump data before request.\n\n\nHave you recalled some familiar scenes?\n\n\nYes! That's what we did in \nrequests.request\n! Since \nHttpRunner\n takes full reuse of \nRequests\n, it inherits all powerful features of \nRequests\n, and we can handle HTTP request as the way we do before.\n\n\nRun test cases\n\n\nSuppose the test case file is named as \nquickstart-demo-rev-0.yml\n and is located in \nexamples\n folder, then we can run it in this way.\n\n\nate examples/demo-rev-0.yml\nRunning tests...\n----------------------------------------------------------------------\n get token ... INFO:root: Start to POST http://127.0.0.1:5000/api/get-token\nINFO:root: status_code: 200, response_time: 48 ms, response_length: 46 bytes\nOK (0.049669)s\n create user which does not exist ... INFO:root: Start to POST http://127.0.0.1:5000/api/users/1000\nERROR:root: Failed to POST http://127.0.0.1:5000/api/users/1000! exception msg: 403 Client Error: FORBIDDEN for url: http://127.0.0.1:5000/api/users/1000\nERROR (0.006471)s\n----------------------------------------------------------------------\nRan 2 tests in 0.056s\n\nFAILED\n (Errors=1)\n\n\n\n\nOops! The second test case failed with 403 status code.\n\n\nThat is because we request with the same data as we captured in \nCharles Proxy\n, while the \ntoken\n is generated dynamically, thus the recorded data can not be be used twice directly.\n\n\nOptimize test case: correlation\n\n\nTo fix this problem, we should correlate \ntoken\n field in the second API test case, which is also called \ncorrelation\n.\n\n\n- test:\n    name: get token\n    request:\n        url: http://127.0.0.1:5000/api/get-token\n        method: POST\n        headers:\n            user_agent: iOS/10.3\n            device_sn: 9TN6O2Bn1vzfybF\n            os_platform: ios\n            app_version: 2.8.6\n        json:\n            sign: 19067cf712265eb5426db8d3664026c1ccea02b9\n    extract:\n        - token: content.token\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 200}\n        - {\"check\": \"content.token\", \"comparator\": \"len_eq\", \"expect\": 16}\n\n- test:\n    name: create user which does not exist\n    request:\n        url: http://127.0.0.1:5000/api/users/1000\n        method: POST\n        headers:\n            device_sn: 9TN6O2Bn1vzfybF\n            token: $token\n        json:\n            name: \"user1\"\n            password: \"123456\"\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 201}\n        - {\"check\": \"content.success\", \"comparator\": \"eq\", \"expect\": true}\n\n\n\n\nAs you see, the \ntoken\n field is no longer hardcoded, instead it is extracted from the first API request with \nextract\n mechanism. In the meanwhile, it is assigned to \ntoken\n variable, which can be referenced by the subsequent API requests.\n\n\nNow we save the test cases to \nquickstart-demo-rev-1.yml\n and rerun it, and we will find that both API requests to be successful.\n\n\nOptimize test case: parameterization\n\n\nLet's look back to our test set \nquickstart-demo-rev-1.yml\n, and we can see the \ndevice_sn\n field is still hardcoded. This may be quite different from the actual scenarios.\n\n\nIn actual scenarios, each user's \ndevice_sn\n is different, so we should parameterize the request parameters, which is also called \nparameterization\n. In the meanwhile, the \nsign\n field is calculated with other header fields, thus it may change significantly if any header field changes slightly.\n\n\nHowever, the test cases are only \nYAML\n documents, it is impossible to generate parameters dynamically in such text. Fortunately, we can combine \nPython\n scripts with \nYAML/JSON\n test cases in \nHttpRunner\n.\n\n\nTo achieve this goal, we can utilize \ndebugtalk.py\n plugin and \nvariables\n mechanisms.\n\n\nTo be specific, we can create a Python file (\nexamples/debugtalk.py\n) and implement the related algorithm in it. The \ndebugtalk.py\n file can not only be located beside \nYAML/JSON\n testset file, but also can be in any upward recursive folder. Since we want \ndebugtalk.py\n to be importable, we should put a \n__init__.py\n in its folder to make it as a Python module.\n\n\nimport hashlib\nimport hmac\nimport random\nimport string\n\nSECRET_KEY = \"DebugTalk\"\n\ndef get_sign(*args):\n    content = ''.join(args).encode('ascii')\n    sign_key = SECRET_KEY.encode('ascii')\n    sign = hmac.new(sign_key, content, hashlib.sha1).hexdigest()\n    return sign\n\ndef gen_random_string(str_len):\n    random_char_list = []\n    for _ in range(str_len):\n        random_char = random.choice(string.ascii_letters + string.digits)\n        random_char_list.append(random_char)\n\n    random_string = ''.join(random_char_list)\n    return random_string\n\n\n\n\nAnd then, we can revise our demo test case and reference the functions. Suppose the revised file named \nquickstart-demo-rev-2.yml\n.\n\n\n- test:\n    name: get token\n    variables:\n        - user_agent: 'iOS/10.3'\n        - device_sn: ${gen_random_string(15)}\n        - os_platform: 'ios'\n        - app_version: '2.8.6'\n    request:\n        url: http://127.0.0.1:5000/api/get-token\n        method: POST\n        headers:\n            user_agent: $user_agent\n            device_sn: $device_sn\n            os_platform: $os_platform\n            app_version: $app_version\n        json:\n            sign: ${get_sign($user_agent, $device_sn, $os_platform, $app_version)}\n    extract:\n        - token: content.token\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 200}\n        - {\"check\": \"content.token\", \"comparator\": \"len_eq\", \"expect\": 16}\n\n- test:\n    name: create user which does not exist\n    request:\n        url: http://127.0.0.1:5000/api/users/1000\n        method: POST\n        headers:\n            device_sn: $device_sn\n            token: $token\n        json:\n            name: \"user1\"\n            password: \"123456\"\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 201}\n        - {\"check\": \"content.success\", \"comparator\": \"eq\", \"expect\": true}\n\n\n\n\nIn this revised test case, \nvariable reference\n and \nfunction invoke\n mechanisms are both used.\n\n\nTo make fields like \ndevice_sn\n can be used more than once, we bind values to variables in \nvariables\n block. When we bind variables, we can not only bind exact value to a variable name, but also can call a function and bind the evaluated value to it.\n\n\nWhen we want to reference a variable in the test case, we can do this with a escape character \n$\n. For example, \n$user_agent\n will not be taken as a normal string, and \nHttpRunner\n will consider it as a variable named \nuser_agent\n, search and return its binding value.\n\n\nWhen we want to reference a function, we shall use another escape character \n${}\n. Any content in \n${}\n will be considered as function calling, so we should guarantee that we call functions in the right way. At the same time, variables can also be referenced as parameters of function.\n\n\nOptimize test case: overall config block\n\n\nThere is still one issue unsolved.\n\n\nThe \ndevice_sn\n field is defined in the first API test case, thus it may be impossible to reference it in other test cases. Context separation is a well-designed mechanism, and we should obey this good practice.\n\n\nTo handle this case, overall \nconfig\n block is supported in \nHttpRunner\n. If we define variables or import functions in \nconfig\n block, these variables and functions will become global and can be referenced in the whole test set.\n\n\n# examples/quickstart-demo-rev-3.yml\n- config:\n    name: \"smoketest for CRUD users.\"\n    variables:\n        - device_sn: ${gen_random_string(15)}\n    request:\n        base_url: http://127.0.0.1:5000\n        headers:\n            device_sn: $device_sn\n\n- test:\n    name: get token\n    variables:\n        - user_agent: 'iOS/10.3'\n        - os_platform: 'ios'\n        - app_version: '2.8.6'\n    request:\n        url: /api/get-token\n        method: POST\n        headers:\n            user_agent: $user_agent\n            os_platform: $os_platform\n            app_version: $app_version\n        json:\n            sign: ${get_sign($user_agent, $device_sn, $os_platform, $app_version)}\n    extract:\n        - token: content.token\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 200}\n        - {\"check\": \"content.token\", \"comparator\": \"len_eq\", \"expect\": 16}\n\n- test:\n    name: create user which does not exist\n    request:\n        url: /api/users/1000\n        method: POST\n        headers:\n            token: $token\n        json:\n            name: \"user1\"\n            password: \"123456\"\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 201}\n        - {\"check\": \"content.success\", \"comparator\": \"eq\", \"expect\": true}\n\n\n\n\nAs you see, we define variables in \nconfig\n block. Also, we can set \nbase_url\n in \nconfig\n block, thereby we can specify relative path in each API request url. Besides, we can also set common fields in \nconfig\n \nrequest\n, such as \ndevice_sn\n in headers.\n\n\nUntil now, the test cases are finished and each detail is handled properly.\n\n\nRun test cases and generate report\n\n\nFinally, let's run test set \nquickstart-demo-rev-3.yml\n once more.\n\n\n$ ate examples/quickstart-demo-rev-4.yml\nRunning tests...\n----------------------------------------------------------------------\n get token ... INFO:root: Start to POST http://127.0.0.1:5000/api/get-token\nINFO:root: status_code: 200, response_time: 33 ms, response_length: 46 bytes\nOK (0.037027)s\n create user which does not exist ... INFO:root: Start to POST http://127.0.0.1:5000/api/users/1000\nINFO:root: status_code: 201, response_time: 15 ms, response_length: 54 bytes\nOK (0.016414)s\n----------------------------------------------------------------------\nRan 2 tests in 0.054s\nOK\n\nGenerating HTML reports...\nTemplate is not specified, load default template instead.\nReports generated: /Users/Leo/MyProjects/HttpRunner/reports/quickstart-demo-rev-0/2017-08-01-16-51-51.html\n\n\n\n\nGreat! The test case runs successfully and generates a \nHTML\n test report.\n\n\n\n\nFurther more\n\n\nThis is just a starting point, see the \nadvanced guide\n for the advanced features.\n\n\n\n\ntemplating\n\n\ndata extraction and validation\n\n\ncomparator",
            "title": "QuickStart"
        },
        {
            "location": "/quickstart/#quickstart",
            "text": "",
            "title": "QuickStart"
        },
        {
            "location": "/quickstart/#introduction-to-sample-interface-service",
            "text": "Along with this project, I devised a sample interface service, and you can use it to familiarize how to play with  HttpRunner .  This sample service mainly has two parts:   Authorization, each request of other APIs should sign with some header fields and get token first.  RESTful APIs for user management, you can do CRUD manipulation on users.   As you see, it is very similar to the mainstream production systems. Therefore once you are familiar with handling this demo service, you can master most test scenarios in your project.",
            "title": "Introduction to Sample Interface Service"
        },
        {
            "location": "/quickstart/#launch-sample-interface-service",
            "text": "The demo service is a flask server, we can launch it in this way.  $ export FLASK_APP=tests/api_server.py\n$ flask run\n * Serving Flask app \"tests.api_server\"\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)  Now the sample interface service is running, and we can move on to the next step.",
            "title": "Launch Sample Interface Service"
        },
        {
            "location": "/quickstart/#capture-http-request-and-response",
            "text": "Before we write testcases, we should know the details of the API. It is a good choice to use a web debugging proxy tool like  Charles Proxy  to capture the HTTP traffic.  For example, the image below illustrates getting token from the sample service first, and then creating one user successfully.    After thorough understanding of the APIs, we can now begin to write testcases.",
            "title": "Capture HTTP request and response"
        },
        {
            "location": "/quickstart/#write-the-first-test-case",
            "text": "Open your favorite text editor and you can write test cases like this.  - test:\n    name: get token\n    request:\n        url: http://127.0.0.1:5000/api/get-token\n        method: POST\n        headers:\n            user_agent: iOS/10.3\n            device_sn: 9TN6O2Bn1vzfybF\n            os_platform: ios\n            app_version: 2.8.6\n        json:\n            sign: 19067cf712265eb5426db8d3664026c1ccea02b9\n\n- test:\n    name: create user which does not exist\n    request:\n        url: http://127.0.0.1:5000/api/users/1000\n        method: POST\n        headers:\n            device_sn: 9TN6O2Bn1vzfybF\n            token: F8prvGryC5beBr4g\n        json:\n            name: \"user1\"\n            password: \"123456\"\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 201}\n        - {\"check\": \"content.success\", \"comparator\": \"eq\", \"expect\": true}  As you see, each API request is described in a  test  block. And in the  request  field, it describes the detail of HTTP request, includes url, method, headers and data, which are in line with the captured traffic.  You may wonder why we use the  json  field other than  data . That's because the post data is in  JSON  format, when we use  json  to indicate the post data, we do not have to specify  Content-Type  to be  application/json  in request headers or dump data before request.  Have you recalled some familiar scenes?  Yes! That's what we did in  requests.request ! Since  HttpRunner  takes full reuse of  Requests , it inherits all powerful features of  Requests , and we can handle HTTP request as the way we do before.",
            "title": "Write the first test case"
        },
        {
            "location": "/quickstart/#run-test-cases",
            "text": "Suppose the test case file is named as  quickstart-demo-rev-0.yml  and is located in  examples  folder, then we can run it in this way.  ate examples/demo-rev-0.yml\nRunning tests...\n----------------------------------------------------------------------\n get token ... INFO:root: Start to POST http://127.0.0.1:5000/api/get-token\nINFO:root: status_code: 200, response_time: 48 ms, response_length: 46 bytes\nOK (0.049669)s\n create user which does not exist ... INFO:root: Start to POST http://127.0.0.1:5000/api/users/1000\nERROR:root: Failed to POST http://127.0.0.1:5000/api/users/1000! exception msg: 403 Client Error: FORBIDDEN for url: http://127.0.0.1:5000/api/users/1000\nERROR (0.006471)s\n----------------------------------------------------------------------\nRan 2 tests in 0.056s\n\nFAILED\n (Errors=1)  Oops! The second test case failed with 403 status code.  That is because we request with the same data as we captured in  Charles Proxy , while the  token  is generated dynamically, thus the recorded data can not be be used twice directly.",
            "title": "Run test cases"
        },
        {
            "location": "/quickstart/#optimize-test-case-correlation",
            "text": "To fix this problem, we should correlate  token  field in the second API test case, which is also called  correlation .  - test:\n    name: get token\n    request:\n        url: http://127.0.0.1:5000/api/get-token\n        method: POST\n        headers:\n            user_agent: iOS/10.3\n            device_sn: 9TN6O2Bn1vzfybF\n            os_platform: ios\n            app_version: 2.8.6\n        json:\n            sign: 19067cf712265eb5426db8d3664026c1ccea02b9\n    extract:\n        - token: content.token\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 200}\n        - {\"check\": \"content.token\", \"comparator\": \"len_eq\", \"expect\": 16}\n\n- test:\n    name: create user which does not exist\n    request:\n        url: http://127.0.0.1:5000/api/users/1000\n        method: POST\n        headers:\n            device_sn: 9TN6O2Bn1vzfybF\n            token: $token\n        json:\n            name: \"user1\"\n            password: \"123456\"\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 201}\n        - {\"check\": \"content.success\", \"comparator\": \"eq\", \"expect\": true}  As you see, the  token  field is no longer hardcoded, instead it is extracted from the first API request with  extract  mechanism. In the meanwhile, it is assigned to  token  variable, which can be referenced by the subsequent API requests.  Now we save the test cases to  quickstart-demo-rev-1.yml  and rerun it, and we will find that both API requests to be successful.",
            "title": "Optimize test case: correlation"
        },
        {
            "location": "/quickstart/#optimize-test-case-parameterization",
            "text": "Let's look back to our test set  quickstart-demo-rev-1.yml , and we can see the  device_sn  field is still hardcoded. This may be quite different from the actual scenarios.  In actual scenarios, each user's  device_sn  is different, so we should parameterize the request parameters, which is also called  parameterization . In the meanwhile, the  sign  field is calculated with other header fields, thus it may change significantly if any header field changes slightly.  However, the test cases are only  YAML  documents, it is impossible to generate parameters dynamically in such text. Fortunately, we can combine  Python  scripts with  YAML/JSON  test cases in  HttpRunner .  To achieve this goal, we can utilize  debugtalk.py  plugin and  variables  mechanisms.  To be specific, we can create a Python file ( examples/debugtalk.py ) and implement the related algorithm in it. The  debugtalk.py  file can not only be located beside  YAML/JSON  testset file, but also can be in any upward recursive folder. Since we want  debugtalk.py  to be importable, we should put a  __init__.py  in its folder to make it as a Python module.  import hashlib\nimport hmac\nimport random\nimport string\n\nSECRET_KEY = \"DebugTalk\"\n\ndef get_sign(*args):\n    content = ''.join(args).encode('ascii')\n    sign_key = SECRET_KEY.encode('ascii')\n    sign = hmac.new(sign_key, content, hashlib.sha1).hexdigest()\n    return sign\n\ndef gen_random_string(str_len):\n    random_char_list = []\n    for _ in range(str_len):\n        random_char = random.choice(string.ascii_letters + string.digits)\n        random_char_list.append(random_char)\n\n    random_string = ''.join(random_char_list)\n    return random_string  And then, we can revise our demo test case and reference the functions. Suppose the revised file named  quickstart-demo-rev-2.yml .  - test:\n    name: get token\n    variables:\n        - user_agent: 'iOS/10.3'\n        - device_sn: ${gen_random_string(15)}\n        - os_platform: 'ios'\n        - app_version: '2.8.6'\n    request:\n        url: http://127.0.0.1:5000/api/get-token\n        method: POST\n        headers:\n            user_agent: $user_agent\n            device_sn: $device_sn\n            os_platform: $os_platform\n            app_version: $app_version\n        json:\n            sign: ${get_sign($user_agent, $device_sn, $os_platform, $app_version)}\n    extract:\n        - token: content.token\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 200}\n        - {\"check\": \"content.token\", \"comparator\": \"len_eq\", \"expect\": 16}\n\n- test:\n    name: create user which does not exist\n    request:\n        url: http://127.0.0.1:5000/api/users/1000\n        method: POST\n        headers:\n            device_sn: $device_sn\n            token: $token\n        json:\n            name: \"user1\"\n            password: \"123456\"\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 201}\n        - {\"check\": \"content.success\", \"comparator\": \"eq\", \"expect\": true}  In this revised test case,  variable reference  and  function invoke  mechanisms are both used.  To make fields like  device_sn  can be used more than once, we bind values to variables in  variables  block. When we bind variables, we can not only bind exact value to a variable name, but also can call a function and bind the evaluated value to it.  When we want to reference a variable in the test case, we can do this with a escape character  $ . For example,  $user_agent  will not be taken as a normal string, and  HttpRunner  will consider it as a variable named  user_agent , search and return its binding value.  When we want to reference a function, we shall use another escape character  ${} . Any content in  ${}  will be considered as function calling, so we should guarantee that we call functions in the right way. At the same time, variables can also be referenced as parameters of function.",
            "title": "Optimize test case: parameterization"
        },
        {
            "location": "/quickstart/#optimize-test-case-overall-config-block",
            "text": "There is still one issue unsolved.  The  device_sn  field is defined in the first API test case, thus it may be impossible to reference it in other test cases. Context separation is a well-designed mechanism, and we should obey this good practice.  To handle this case, overall  config  block is supported in  HttpRunner . If we define variables or import functions in  config  block, these variables and functions will become global and can be referenced in the whole test set.  # examples/quickstart-demo-rev-3.yml\n- config:\n    name: \"smoketest for CRUD users.\"\n    variables:\n        - device_sn: ${gen_random_string(15)}\n    request:\n        base_url: http://127.0.0.1:5000\n        headers:\n            device_sn: $device_sn\n\n- test:\n    name: get token\n    variables:\n        - user_agent: 'iOS/10.3'\n        - os_platform: 'ios'\n        - app_version: '2.8.6'\n    request:\n        url: /api/get-token\n        method: POST\n        headers:\n            user_agent: $user_agent\n            os_platform: $os_platform\n            app_version: $app_version\n        json:\n            sign: ${get_sign($user_agent, $device_sn, $os_platform, $app_version)}\n    extract:\n        - token: content.token\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 200}\n        - {\"check\": \"content.token\", \"comparator\": \"len_eq\", \"expect\": 16}\n\n- test:\n    name: create user which does not exist\n    request:\n        url: /api/users/1000\n        method: POST\n        headers:\n            token: $token\n        json:\n            name: \"user1\"\n            password: \"123456\"\n    validate:\n        - {\"check\": \"status_code\", \"comparator\": \"eq\", \"expect\": 201}\n        - {\"check\": \"content.success\", \"comparator\": \"eq\", \"expect\": true}  As you see, we define variables in  config  block. Also, we can set  base_url  in  config  block, thereby we can specify relative path in each API request url. Besides, we can also set common fields in  config   request , such as  device_sn  in headers.  Until now, the test cases are finished and each detail is handled properly.",
            "title": "Optimize test case: overall config block"
        },
        {
            "location": "/quickstart/#run-test-cases-and-generate-report",
            "text": "Finally, let's run test set  quickstart-demo-rev-3.yml  once more.  $ ate examples/quickstart-demo-rev-4.yml\nRunning tests...\n----------------------------------------------------------------------\n get token ... INFO:root: Start to POST http://127.0.0.1:5000/api/get-token\nINFO:root: status_code: 200, response_time: 33 ms, response_length: 46 bytes\nOK (0.037027)s\n create user which does not exist ... INFO:root: Start to POST http://127.0.0.1:5000/api/users/1000\nINFO:root: status_code: 201, response_time: 15 ms, response_length: 54 bytes\nOK (0.016414)s\n----------------------------------------------------------------------\nRan 2 tests in 0.054s\nOK\n\nGenerating HTML reports...\nTemplate is not specified, load default template instead.\nReports generated: /Users/Leo/MyProjects/HttpRunner/reports/quickstart-demo-rev-0/2017-08-01-16-51-51.html  Great! The test case runs successfully and generates a  HTML  test report.",
            "title": "Run test cases and generate report"
        },
        {
            "location": "/quickstart/#further-more",
            "text": "This is just a starting point, see the  advanced guide  for the advanced features.   templating  data extraction and validation  comparator",
            "title": "Further more"
        },
        {
            "location": "/write-testcases/",
            "text": "It is recommended to write testcases in \nYAML\n format.\n\n\ndemo\n\n\nHere is a testset example of typical scenario: get \ntoken\n at the beginning, and each subsequent requests should take the \ntoken\n in the headers.\n\n\n- config:\n    name: \"create user testsets.\"\n    variables:\n        - user_agent: 'iOS/10.3'\n        - device_sn: ${gen_random_string(15)}\n        - os_platform: 'ios'\n        - app_version: '2.8.6'\n    request:\n        base_url: \"http://127.0.0.1:5000\"\n        headers:\n            Content-Type: application/json\n            device_sn: $device_sn\n\n- test:\n    name: get token\n    request:\n        url: /api/get-token\n        method: POST\n        headers:\n            user_agent: $user_agent\n            device_sn: $device_sn\n            os_platform: $os_platform\n            app_version: $app_version\n        json:\n            sign: ${get_sign($user_agent, $device_sn, $os_platform, $app_version)}\n    extract:\n        - token: content.token\n    validate:\n        - eq: [\"status_code\", 200]\n        - len_eq: [\"content.token\", 16]\n\n- test:\n    name: create user which does not exist\n    request:\n        url: /api/users/1000\n        method: POST\n        headers:\n            token: $token\n        json:\n            name: \"user1\"\n            password: \"123456\"\n    validate:\n        - eq: [\"status_code\", 201]\n        - eq: [\"content.success\", true]\n\n\n\n\nFunction invoke is supported in \nYAML/JSON\n format testcases, such as \ngen_random_string\n and \nget_sign\n above. This mechanism relies on the \ndebugtak.py\n hot plugin, with which we can define functions in \ndebugtak.py\n file, and then functions can be auto discovered and invoked in runtime.\n\n\nFor detailed regulations of writing testcases, you can read the \nquickstart\n documents.\n\n\nComparator\n\n\nHttpRunner\n currently supports the following comparators.\n\n\n\n\n\n\n\n\ncomparator\n\n\nDescription\n\n\nA(check), B(expect)\n\n\nexamples\n\n\n\n\n\n\n\n\n\n\neq\n, \n==\n\n\nvalue is equal\n\n\nA == B\n\n\n9 eq 9\n\n\n\n\n\n\nlt\n\n\nless than\n\n\nA < B\n\n\n7 lt 8\n\n\n\n\n\n\nle\n\n\nless than or equals\n\n\nA <= B\n\n\n7 le 8, 8 le 8\n\n\n\n\n\n\ngt\n\n\ngreater than\n\n\nA > B\n\n\n8 gt 7\n\n\n\n\n\n\nge\n\n\ngreater than or equals\n\n\nA >= B\n\n\n8 ge 7, 8 ge 8\n\n\n\n\n\n\nne\n\n\nnot equals\n\n\nA != B\n\n\n6 ne 9\n\n\n\n\n\n\nstr_eq\n\n\nstring equals\n\n\nstr(A) == str(B)\n\n\n123 str_eq '123'\n\n\n\n\n\n\nlen_eq\n, \ncount_eq\n\n\nlength or count equals\n\n\nlen(A) == B\n\n\n'abc' len_eq 3, [1,2] len_eq 2\n\n\n\n\n\n\nlen_gt\n, \ncount_gt\n\n\nlength greater than\n\n\nlen(A) > B\n\n\n'abc' len_gt 2, [1,2,3] len_gt 2\n\n\n\n\n\n\nlen_ge\n, \ncount_ge\n\n\nlength greater than or equals\n\n\nlen(A) >= B\n\n\n'abc' len_ge 3, [1,2,3] len_gt 3\n\n\n\n\n\n\nlen_lt\n, \ncount_lt\n\n\nlength less than\n\n\nlen(A) < B\n\n\n'abc' len_lt 4, [1,2,3] len_lt 4\n\n\n\n\n\n\nlen_le\n, \ncount_le\n\n\nlength less than or equals\n\n\nlen(A) <= B\n\n\n'abc' len_le 3, [1,2,3] len_le 3\n\n\n\n\n\n\ncontains\n\n\ncontains\n\n\n[1, 2] contains 1\n\n\n'abc' contains 'a', [1,2,3] len_lt 4\n\n\n\n\n\n\ncontained_by\n\n\ncontained by\n\n\nA in B\n\n\n'a' contained_by 'abc', 1 contained_by [1,2]\n\n\n\n\n\n\ntype_match\n\n\nA is instance of B\n\n\nisinstance(A, B)\n\n\n123 type_match 'int'\n\n\n\n\n\n\nregex_match\n\n\nregex matches\n\n\nre.match(B, A)\n\n\n'abcdef' regex 'a\\w+d'\n\n\n\n\n\n\nstartswith\n\n\nstarts with\n\n\nA.startswith(B) is True\n\n\n'abc' startswith 'ab'\n\n\n\n\n\n\nendswith\n\n\nends with\n\n\nA.endswith(B) is True\n\n\n'abc' endswith 'bc'\n\n\n\n\n\n\n\n\nExtraction and Validation\n\n\nSuppose we get the following HTTP response.\n\n\n// status code: 200\n\n// response headers\n{\n    \"Content-Type\": \"application/json\"\n}\n\n// response body content\n{\n    \"success\": False,\n    \"person\": {\n        \"name\": {\n            \"first_name\": \"Leo\",\n            \"last_name\": \"Lee\",\n        },\n        \"age\": 29,\n        \"cities\": [\"Guangzhou\", \"Shenzhen\"]\n    }\n}\n\n\n\n\nIn \nextract\n and \nvalidate\n, we can do chain operation to extract data field in HTTP response.\n\n\nFor instance, if we want to get \nContent-Type\n in response headers, then we can specify \nheaders.content-type\n; if we want to get \nfirst_name\n in response content, we can specify \ncontent.person.name.first_name\n.\n\n\nThere might be slight difference on list, cos we can use index to locate list item. For example, \nGuangzhou\n in response content can be specified by \ncontent.person.cities.0\n.\n\n\n// get status code\nstatus_code\n\n// get headers field\nheaders.content-type\n\n// get content field\nbody.success\ncontent.success\ntext.success\ncontent.person.name.first_name\ncontent.person.cities.1\n\n\n\n\nextract:\n    - content_type: headers.content-type\n    - first_name: content.person.name.first_name\nvalidate:\n    - eq: [\"status_code\", 200]\n    - eq: [\"headers.content-type\", \"application/json\"]\n    - gt: [\"headers.content-length\", 40]\n    - eq: [\"content.success\", true]\n    - len_eq: [\"content.token\", 16]",
            "title": "Write testcases"
        },
        {
            "location": "/write-testcases/#demo",
            "text": "Here is a testset example of typical scenario: get  token  at the beginning, and each subsequent requests should take the  token  in the headers.  - config:\n    name: \"create user testsets.\"\n    variables:\n        - user_agent: 'iOS/10.3'\n        - device_sn: ${gen_random_string(15)}\n        - os_platform: 'ios'\n        - app_version: '2.8.6'\n    request:\n        base_url: \"http://127.0.0.1:5000\"\n        headers:\n            Content-Type: application/json\n            device_sn: $device_sn\n\n- test:\n    name: get token\n    request:\n        url: /api/get-token\n        method: POST\n        headers:\n            user_agent: $user_agent\n            device_sn: $device_sn\n            os_platform: $os_platform\n            app_version: $app_version\n        json:\n            sign: ${get_sign($user_agent, $device_sn, $os_platform, $app_version)}\n    extract:\n        - token: content.token\n    validate:\n        - eq: [\"status_code\", 200]\n        - len_eq: [\"content.token\", 16]\n\n- test:\n    name: create user which does not exist\n    request:\n        url: /api/users/1000\n        method: POST\n        headers:\n            token: $token\n        json:\n            name: \"user1\"\n            password: \"123456\"\n    validate:\n        - eq: [\"status_code\", 201]\n        - eq: [\"content.success\", true]  Function invoke is supported in  YAML/JSON  format testcases, such as  gen_random_string  and  get_sign  above. This mechanism relies on the  debugtak.py  hot plugin, with which we can define functions in  debugtak.py  file, and then functions can be auto discovered and invoked in runtime.  For detailed regulations of writing testcases, you can read the  quickstart  documents.",
            "title": "demo"
        },
        {
            "location": "/write-testcases/#comparator",
            "text": "HttpRunner  currently supports the following comparators.     comparator  Description  A(check), B(expect)  examples      eq ,  ==  value is equal  A == B  9 eq 9    lt  less than  A < B  7 lt 8    le  less than or equals  A <= B  7 le 8, 8 le 8    gt  greater than  A > B  8 gt 7    ge  greater than or equals  A >= B  8 ge 7, 8 ge 8    ne  not equals  A != B  6 ne 9    str_eq  string equals  str(A) == str(B)  123 str_eq '123'    len_eq ,  count_eq  length or count equals  len(A) == B  'abc' len_eq 3, [1,2] len_eq 2    len_gt ,  count_gt  length greater than  len(A) > B  'abc' len_gt 2, [1,2,3] len_gt 2    len_ge ,  count_ge  length greater than or equals  len(A) >= B  'abc' len_ge 3, [1,2,3] len_gt 3    len_lt ,  count_lt  length less than  len(A) < B  'abc' len_lt 4, [1,2,3] len_lt 4    len_le ,  count_le  length less than or equals  len(A) <= B  'abc' len_le 3, [1,2,3] len_le 3    contains  contains  [1, 2] contains 1  'abc' contains 'a', [1,2,3] len_lt 4    contained_by  contained by  A in B  'a' contained_by 'abc', 1 contained_by [1,2]    type_match  A is instance of B  isinstance(A, B)  123 type_match 'int'    regex_match  regex matches  re.match(B, A)  'abcdef' regex 'a\\w+d'    startswith  starts with  A.startswith(B) is True  'abc' startswith 'ab'    endswith  ends with  A.endswith(B) is True  'abc' endswith 'bc'",
            "title": "Comparator"
        },
        {
            "location": "/write-testcases/#extraction-and-validation",
            "text": "Suppose we get the following HTTP response.  // status code: 200\n\n// response headers\n{\n    \"Content-Type\": \"application/json\"\n}\n\n// response body content\n{\n    \"success\": False,\n    \"person\": {\n        \"name\": {\n            \"first_name\": \"Leo\",\n            \"last_name\": \"Lee\",\n        },\n        \"age\": 29,\n        \"cities\": [\"Guangzhou\", \"Shenzhen\"]\n    }\n}  In  extract  and  validate , we can do chain operation to extract data field in HTTP response.  For instance, if we want to get  Content-Type  in response headers, then we can specify  headers.content-type ; if we want to get  first_name  in response content, we can specify  content.person.name.first_name .  There might be slight difference on list, cos we can use index to locate list item. For example,  Guangzhou  in response content can be specified by  content.person.cities.0 .  // get status code\nstatus_code\n\n// get headers field\nheaders.content-type\n\n// get content field\nbody.success\ncontent.success\ntext.success\ncontent.person.name.first_name\ncontent.person.cities.1  extract:\n    - content_type: headers.content-type\n    - first_name: content.person.name.first_name\nvalidate:\n    - eq: [\"status_code\", 200]\n    - eq: [\"headers.content-type\", \"application/json\"]\n    - gt: [\"headers.content-length\", 40]\n    - eq: [\"content.success\", true]\n    - len_eq: [\"content.token\", 16]",
            "title": "Extraction and Validation"
        },
        {
            "location": "/run-testcases/",
            "text": "Run testcases\n\n\nHttpRunner\n can run testcases in diverse ways.\n\n\nYou can run single testset by specifying testset file path.\n\n\n$ httprunner filepath/testcase.yml\n\n\n\n\nYou can also run several testsets by specifying multiple testset file paths.\n\n\n$ httprunner filepath1/testcase1.yml filepath2/testcase2.yml\n\n\n\n\nIf you want to run testsets of a whole project, you can achieve this goal by specifying the project folder path.\n\n\n$ httprunner testcases_folder_path\n\n\n\n\nWhen you do continuous integration test or production environment monitoring with \nJenkins\n, you may need to send test result notification. For instance, you can send email with mailgun service as below.\n\n\n$ httprunner filepath/testcase.yml --report-name ${BUILD_NUMBER} \\\n    --mailgun-smtp-username \"qa@debugtalk.com\" \\\n    --mailgun-smtp-password \"12345678\" \\\n    --email-sender excited@samples.mailgun.org \\\n    --email-recepients ${MAIL_RECEPIENTS} \\\n    --jenkins-job-name ${JOB_NAME} \\\n    --jenkins-job-url ${JOB_URL} \\\n    --jenkins-build-number ${BUILD_NUMBER}",
            "title": "Run testcases"
        },
        {
            "location": "/run-testcases/#run-testcases",
            "text": "HttpRunner  can run testcases in diverse ways.  You can run single testset by specifying testset file path.  $ httprunner filepath/testcase.yml  You can also run several testsets by specifying multiple testset file paths.  $ httprunner filepath1/testcase1.yml filepath2/testcase2.yml  If you want to run testsets of a whole project, you can achieve this goal by specifying the project folder path.  $ httprunner testcases_folder_path  When you do continuous integration test or production environment monitoring with  Jenkins , you may need to send test result notification. For instance, you can send email with mailgun service as below.  $ httprunner filepath/testcase.yml --report-name ${BUILD_NUMBER} \\\n    --mailgun-smtp-username \"qa@debugtalk.com\" \\\n    --mailgun-smtp-password \"12345678\" \\\n    --email-sender excited@samples.mailgun.org \\\n    --email-recepients ${MAIL_RECEPIENTS} \\\n    --jenkins-job-name ${JOB_NAME} \\\n    --jenkins-job-url ${JOB_URL} \\\n    --jenkins-build-number ${BUILD_NUMBER}",
            "title": "Run testcases"
        },
        {
            "location": "/load-test/",
            "text": "Load Test\n\n\nWith reuse of \nLocust\n, you can run performance test without extra work.\n\n\n$ locusts -V\n[2017-08-26 23:45:42,246] bogon/INFO/stdout: Locust 0.8a2\n[2017-08-26 23:45:42,246] bogon/INFO/stdout:\n\n\n\n\nFor full usage, you can run \nlocusts -h\n to see help, and you will find that it is the same with \nlocust -h\n.\n\n\nThe only difference is the \n-f\n argument. If you specify \n-f\n with a Python locustfile, it will be the same as \nlocust\n, while if you specify \n-f\n with a \nYAML/JSON\n testcase file, it will convert to Python locustfile first and then pass to \nlocust\n.\n\n\n$ locusts -f examples/first-testcase.yml\n[2017-08-18 17:20:43,915] Leos-MacBook-Air.local/INFO/locust.main: Starting web monitor at *:8089\n[2017-08-18 17:20:43,918] Leos-MacBook-Air.local/INFO/locust.main: Starting Locust 0.8a2\n\n\n\n\nIn this case, you can reuse all features of \nLocust\n.\n\n\nThat\u2019s not all about it. With the argument \n--cpu-cores\n, you can even start locust with master and specified number of slaves (default to cpu cores number) at one time, which means you can leverage all cpus of your machine.\n\n\n$ locusts -f examples/first-testcase.yml --cpu-cores 4\n[2017-08-26 23:51:47,071] bogon/INFO/locust.main: Starting web monitor at *:8089\n[2017-08-26 23:51:47,075] bogon/INFO/locust.main: Starting Locust 0.8a2\n[2017-08-26 23:51:47,078] bogon/INFO/locust.main: Starting Locust 0.8a2\n[2017-08-26 23:51:47,080] bogon/INFO/locust.main: Starting Locust 0.8a2\n[2017-08-26 23:51:47,083] bogon/INFO/locust.main: Starting Locust 0.8a2\n[2017-08-26 23:51:47,084] bogon/INFO/locust.runners: Client 'bogon_656e0af8e968a8533d379dd252422ad3' reported as ready. Currently 1 clients ready to swarm.\n[2017-08-26 23:51:47,085] bogon/INFO/locust.runners: Client 'bogon_09f73850252ee4ec739ed77d3c4c6dba' reported as ready. Currently 2 clients ready to swarm.\n[2017-08-26 23:51:47,084] bogon/INFO/locust.main: Starting Locust 0.8a2\n[2017-08-26 23:51:47,085] bogon/INFO/locust.runners: Client 'bogon_869f7ed671b1a9952b56610f01e2006f' reported as ready. Currently 3 clients ready to swarm.\n[2017-08-26 23:51:47,085] bogon/INFO/locust.runners: Client 'bogon_80a804cda36b80fac17b57fd2d5e7cdb' reported as ready. Currently 4 clients ready to swarm.\n\n\n\n\n\n\nEnjoy!",
            "title": "Load test"
        },
        {
            "location": "/load-test/#load-test",
            "text": "With reuse of  Locust , you can run performance test without extra work.  $ locusts -V\n[2017-08-26 23:45:42,246] bogon/INFO/stdout: Locust 0.8a2\n[2017-08-26 23:45:42,246] bogon/INFO/stdout:  For full usage, you can run  locusts -h  to see help, and you will find that it is the same with  locust -h .  The only difference is the  -f  argument. If you specify  -f  with a Python locustfile, it will be the same as  locust , while if you specify  -f  with a  YAML/JSON  testcase file, it will convert to Python locustfile first and then pass to  locust .  $ locusts -f examples/first-testcase.yml\n[2017-08-18 17:20:43,915] Leos-MacBook-Air.local/INFO/locust.main: Starting web monitor at *:8089\n[2017-08-18 17:20:43,918] Leos-MacBook-Air.local/INFO/locust.main: Starting Locust 0.8a2  In this case, you can reuse all features of  Locust .  That\u2019s not all about it. With the argument  --cpu-cores , you can even start locust with master and specified number of slaves (default to cpu cores number) at one time, which means you can leverage all cpus of your machine.  $ locusts -f examples/first-testcase.yml --cpu-cores 4\n[2017-08-26 23:51:47,071] bogon/INFO/locust.main: Starting web monitor at *:8089\n[2017-08-26 23:51:47,075] bogon/INFO/locust.main: Starting Locust 0.8a2\n[2017-08-26 23:51:47,078] bogon/INFO/locust.main: Starting Locust 0.8a2\n[2017-08-26 23:51:47,080] bogon/INFO/locust.main: Starting Locust 0.8a2\n[2017-08-26 23:51:47,083] bogon/INFO/locust.main: Starting Locust 0.8a2\n[2017-08-26 23:51:47,084] bogon/INFO/locust.runners: Client 'bogon_656e0af8e968a8533d379dd252422ad3' reported as ready. Currently 1 clients ready to swarm.\n[2017-08-26 23:51:47,085] bogon/INFO/locust.runners: Client 'bogon_09f73850252ee4ec739ed77d3c4c6dba' reported as ready. Currently 2 clients ready to swarm.\n[2017-08-26 23:51:47,084] bogon/INFO/locust.main: Starting Locust 0.8a2\n[2017-08-26 23:51:47,085] bogon/INFO/locust.runners: Client 'bogon_869f7ed671b1a9952b56610f01e2006f' reported as ready. Currently 3 clients ready to swarm.\n[2017-08-26 23:51:47,085] bogon/INFO/locust.runners: Client 'bogon_80a804cda36b80fac17b57fd2d5e7cdb' reported as ready. Currently 4 clients ready to swarm.   Enjoy!",
            "title": "Load Test"
        },
        {
            "location": "/development/",
            "text": "Development\n\n\nTo develop or debug \nHttpRunner\n, you can install relevant requirements and use \nmain-hrun.py\n or \nmain-locust.py\n as entrances.\n\n\n$ pip install -r requirements.txt\n$ python main-hrun -h\n$ python main-locust -h",
            "title": "Development"
        },
        {
            "location": "/development/#development",
            "text": "To develop or debug  HttpRunner , you can install relevant requirements and use  main-hrun.py  or  main-locust.py  as entrances.  $ pip install -r requirements.txt\n$ python main-hrun -h\n$ python main-locust -h",
            "title": "Development"
        },
        {
            "location": "/FAQ/",
            "text": "Unable to install PyUnitReport dependency library automatically\n\n\nIf there is something goes wrong in installation like below.\n\n\nDownloading/unpacking PyUnitReport (from HttpRunner)\n  Could not find any downloads that satisfy the requirement PyUnitReport (from HttpRunner)\n\n\n\n\nYou could install \nPyUnitReport\n manully at first.\n\n\n$ pip install git+https://github.com/debugtalk/PyUnitReport.git#egg=PyUnitReport\n\n\n\n\nAnd then everything will be OK when you reinstall \nHttpRunner\n.\n\n\n$ pip install git+https://github.com/debugtalk/HttpRunner.git#egg=HttpRunner",
            "title": "FAQ"
        },
        {
            "location": "/FAQ/#unable-to-install-pyunitreport-dependency-library-automatically",
            "text": "If there is something goes wrong in installation like below.  Downloading/unpacking PyUnitReport (from HttpRunner)\n  Could not find any downloads that satisfy the requirement PyUnitReport (from HttpRunner)  You could install  PyUnitReport  manully at first.  $ pip install git+https://github.com/debugtalk/PyUnitReport.git#egg=PyUnitReport  And then everything will be OK when you reinstall  HttpRunner .  $ pip install git+https://github.com/debugtalk/HttpRunner.git#egg=HttpRunner",
            "title": "Unable to install PyUnitReport dependency library automatically"
        }
    ]
}